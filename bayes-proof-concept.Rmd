---
title: "Bayesian Psychometrics for Dynamic Learning Maps: A Proof of Concept"
shorttitle: "Bayesian Modeling for DLM"
subtitle: "Technical Report #19-02"
date: "`r format(Sys.Date(), '%B %Y')`"
knit: "bookdown::render_book"
site: bookdown::bookdown_site
output: ratlas::techreport_pdf
bibliography: ["bib/refs.bib", "bib/packages.bib"]
biblio-style: apa
biblatexoptions:
  - sortcites
csl: csl/apa.csl
link-citations: yes
lot: true
lof: true
subparagraph: yes
mainfont: Palatino LT Std
fontsize: 11pt
acknowledgements: >
  `r if (knitr::is_latex_output()) ratlas::inc("front-matter/preface.Rmd")`
---

```{r setup, include=FALSE}
needed_packages <- c("ratlas", "knitr", "english", "kableExtra", "tidyverse",
                     "rstan", "loo", "tidybayes",
                     "here", "glue", "fs")
load_packages <- function(x) {
  if (!(x %in% installed.packages())) {
    install.packages(x, repos = "https://cran.rstudio.com/")
  }
  
  suppressPackageStartupMessages(require(x, character.only = TRUE))
}
vapply(needed_packages, load_packages, logical(1))

extrafont::loadfonts(quiet = TRUE)
ratlas::update_geom_font_ms_defaults()

options(knitr.kable.NA = "")
```

```{r functions}
logit <- function(x) {
  log(x / (1 - x))
}
inv_logit <- function(x) {
  exp(x) / (1 + exp(x))
}
trunc_sample <- function(func, n, lb = -Inf, ub = Inf, ...) {
  full_sample <- func(n = n, ...)
  trunc_sample <- full_sample[between(full_sample, lb, ub)]
  
  while(length(trunc_sample) < n) {
    full_sample <- func(n = n, ...)
    trunc_sample <- c(trunc_sample, full_sample[between(full_sample, lb, ub)])
  }
  
  sample(trunc_sample, size = n, replace = FALSE)
}
rep_data <- function(model, obs) {
  draws <- model %>%
    spread_draws(nu[c], pi[i,c]) %>% 
    ungroup()
  
  tidy_draws <- full_join(
    draws %>%
      select(.chain:.draw, nu, c) %>%
      distinct() %>%
      spread(key = c, value = nu) %>%
      rename(nm_nu = `1`, ms_nu = `2`),
    draws %>%
      select(.chain:.draw, pi, i, c) %>%
      distinct() %>%
      spread(key = c, value = pi) %>%
      rename(nm_pi = `1`, ms_pi = `2`),
    by = c(".chain", ".iteration", ".draw")
  )
  
  replicated_data <- tidy_draws %>%
    group_nest(.chain, .iteration, .draw, .key = "params") %>%
    mutate(data_rep = map(params, function(x, obs) {
      master_probs <- obs %>%
        left_join(x, by = c("item_id" = "i")) %>%
        mutate(log_nm = (score * log(nm_pi)) + ((1 - score) * log(1 - nm_pi)),
               log_ms = (score * log(ms_pi)) + ((1 - score) * log(1 - ms_pi))) %>%
        group_by(stu_id, nm_nu, ms_nu) %>%
        summarize(log_nm = sum(log_nm), log_ms = sum(log_ms)) %>%
        ungroup() %>%
        mutate(prob_nm = nm_nu * exp(log_nm),
               prob_ms = ms_nu * exp(log_ms),
               master_prob = prob_ms / (prob_nm + prob_ms)) %>%
        select(stu_id, master_prob)
      
      ppmc_data <- obs %>%
        select(-score) %>%
        left_join(master_probs, by = "stu_id") %>%
        left_join(select(x, i, nm_pi, ms_pi), by = c("item_id" = "i")) %>%
        mutate(prob_correct = case_when(master_prob >= 0.5 ~ ms_pi,
                                        TRUE ~ nm_pi)) %>%
        # mutate(prob_correct = (master_prob * ms_pi) +
        #          ((1 - master_prob) * nm_pi)) %>%
        mutate(rand = runif(n = nrow(.), min = 0, max = 1),
               score = case_when(rand <= prob_correct ~ 1L,
                                 TRUE ~ 0L)) %>%
        select(stu_id, item_id, score)
      
      ret_df <- tibble(
        mastery_probs = list(master_probs),
        ppmc_data = list(ppmc_data)
      )
      
      return(ret_df)
    }, obs = obs)) %>%
    unnest(data_rep)
  
  return(replicated_data)
}
ggsave2 <- function(plot, filename, width = 8, height = NULL, dir = "h", ...) {
  if (is.null(height)) {
    asp <- ifelse(dir == "h", 0.618, 1.618)
    height <- width * asp
  }
  
  ggplot2::ggsave(filename, plot, width = width, height = height, units = "in",
                  dpi = "retina", ...)
  invisible(plot)
}
```

# Executive Summary {-}

Placeholder text.

# Implications for the Field {-}

Placeholder text.

\newpage

# Purpose of the Report

The Dynamic Learning Maps^&reg;^ (DLM^&reg;^) Alternate Assessment System utilizes diagnostic classification models (DCMs) to scale and score assessments in English language arts (ELA), mathematics, and science. DCMs are able to provide fine-grained and actionable scores for a set of assessed skills or attributes [@bradshaw_dcm]. However, because this class of models is relatively new to operational use, many psychometric properties require further investigation to support the use of the assessments. One key feature that is not well defined in the literature is how best to asses the model fit of DCMs [@rupp_dcm]. In this report, the limitations of the current model fit methodology are discussed, and proof of concept is presented for using Bayesian estimation and posterior predictive model checks for the evaluation of model fit for DCMs.

# Limitations of the Current Approach

The DLM assessments currently estimate each linkage level as a separate latent class analysis using an expectation-maximization algorithm [@bartholomew_lca; @dlm_tech_1516_im]. Prior work on this modeling approach has demonstrated that although this methodology is able to estimate and score the assessment, it is unable to adequately address issues of psychometric concern, primarily those related to model fit [@rupp_dcm]. Under the current modeling approach, evaluation of model fit relies primarily on univariate item tests, meaning that higher order evaluations of model fit using multiple items are not readily calculable. Further, the indices that are able to be computed rely on $\chi^2$ tests that are known to be asymptotically incorrect [@maydeu_2005; @maydeu_2006].

Due to these concerns, this document investigates a Bayesian approach to the estimation of the latent class model. This approach allows for the estimation of alternative methods for the evaluation of model fit through posterior predictive model checking.

# Defining the Bayesian Model

The general form of DCMs can be seen in equation \@ref(eq:dcm), where the probability of respondent $j$ providing a given item response can be modeled as:

\begin{equation}
  P(\text{X}_j = \text{x}_j) = \sum_{c=1}^C\nu_c\prod_{i=1}^{I}\pi_{ic}^{x_{ij}}(1 - \pi_{ic})^{1-x_{ij}}
  (\#eq:dcm)
\end{equation}

In equation \@ref(eq:dcm), $\pi_{ic}$ is the probability of a respondent in class $c$ providing a correct response to item $i$, and $x_{ij}$ is the observed response (i.e., 0, 1) of respondent $j$ to item $i$. Thus, $\pi_{ic}^{x_{ij}}(1 - \pi_{ic})^{1-x_{ij}}$ represents the probability of a respondent in class $c$ providing the observed response to item $i$. These probabilities are then multipled across all items, giving the probability of a respondent in class $c$ providing a the observed response pattern. Finally, this probability is multiple by $\nu_c$, which is the base rate probability that any given respondent belong to class $c$. Thus, this product represents the probability that a given respondent is in class $c$ and provided the observed response pattern.

For DCMs with binary attributes (e.g., master and non-master), there are $2^A$ mastery profiles that a respondent could be classified into. Because DLM uses a simplified latent class model, there is only one attribute per model, and thus two classes.

Where different classes of DCMs differ is in how $\pi_{ic}$ is defined. For example, the log-linear cognitive diagnosis model [LCDM; @lcdm] defines $\pi_{ic}$ similar to generalized linear models with a logit link function. Specifically, $\pi_{ic}$ is defined as seen in equation \@ref(eq:meas-lcdm), where $\alpha_c$ is a binary indicator of the mastery status for a respondent in class $c$.

\begin{equation}
  \pi_{ic} = P(\text{X}_{ic}=1\ |\ \alpha_c) = \frac{\exp(\lambda_{i,0} + \lambda_{i,1,1}\alpha_c)}{1 + \exp(\lambda_{i,0} + \lambda_{i,1,1}\alpha_c)}
  (\#eq:meas-lcdm)
\end{equation}

When using this notation introduced by @rupp_dcm, the $\lambda$ subscripts follow the order of: item, effect, attribute. That is, the first subscript identifies the item for the parameter (noted as $i$). The second subscript denotes the type of effect. Because this discussion is limited to single attribute models, there are only two types of effects where 0 identifies an intercept and 1 a main effect. In models with multiple attributes, there may be additional effects for two-, three-, or *A*-way interactions. Finally, the last element of the subscript identifies the attribute(s). Again, as these are single attribute models, this element is either non-existent (for intercept terms where no attribute is involved) or 1 (for all other effects). It is included here only for consistency with the notation in @rupp_dcm.

When implemented for the DLM assessments, equation \@ref(eq:meas-lcdm) is modified slightly in order to include both attribute- and item-level effects, similar to multilevel models:

\begin{equation}
  \pi_{ic} = P(\text{X}_{ic}=1\ |\ \alpha_c) = \frac{\exp[\lambda_{0} + b_{i,0} + (\lambda_{1,1} + b_{i,1,1})\alpha_c]}{1 + \exp[\lambda_{0} + b_{i,0} + (\lambda_{1,1} + b_{i,1,1})\alpha_c]}
  (\#eq:dlm-lcdm)
\end{equation}

Equation \@ref(eq:dlm-lcdm) shows the similarity to multilevel models. In this model, $\lambda_0$ and $\lambda_{1,1}$ represent the attribute level intercept and main effect respectively. These are akin to the average intercept and main effect for all items (the fixed effects in the multilevel model literature). In addition to the attribute level parameters, there are also item-level intercepts ($b_{i,0}$) and main effects ($b_{i,1,1}$). These parameters represent the deviation from the attribute level effect for each item. Thus, the full intercept for item one would be calculated as $\lambda_0 + b_{1,0}$. This is similar to the estimation of random intercepts and slopes for each item [@stroup_glmm]. The difference between the proposed model and multilevel models is the treatment of the variance of these item-level parameters. In multilevel models, the variance of these effects would be estimated. However, for this model, the variance of these effects is fixed to pre-specified values.

If the item-level parameters are constrained to be 0, then all items will have parameters equal to the attribute-level parameter (i.e., all of the $b_{i,0}$ and $b_{i,1,1}$ parameters would be zero). This is mathematically equivalent to the fungible model that is currently used as the operational model for DLM [@dlm_tech_1516_im]. Alternatively, the item-level parameters can be allowed to vary freely with no constraints (i.e., a non-fungible model). Finally, a non-flat prior can be placed on the item-level parameters, such that the parameters are not constrained to be 0, but also not allowed to vary completely freely either.

## Prior Specification for Attribute-Level Effects {#attr-priors}

In equation \@ref(eq:dlm-lcdm), there are two attribute-level effects that require prior specifications. The first attribute-level effect is $\lambda_{0}$, which represents the average intercept across all items. Thus, this parameter also represents the log-odds (due to the logit link function) of a non-master providing a correct response to an average item. For this parameter, a $\mathcal{N}(\mu = 0,\ \sigma=2)$ distribution was used as the prior. This prior distribution was chosen as 99% of the distribution encompasses the plausible values for this parameter. Specifically, the middle 99% of the distribution consists of the log-odds range -5.15 to 5.15, which covers nearly all of the probability scale with other parameters are equal to zero, as seen in Figure \@ref(fig:log-odds).

```{r log-odds, fig.cap = "Log-odds to probability conversion."}
tibble(x = seq(-5, 5, by = 0.01)) %>%
  mutate(y = 1 / (1 + exp(-x))) %>%
  ggplot(aes(x = x, y = y)) +
    geom_line() +
    scale_x_continuous(breaks = seq(-8, 8, by = 2)) +
    labs(x = "Log-odds", y = "Probability") +
    theme_atlas_ms() -> plot

plot %>%
  ggsave2(fig_path(".png")) %>%
  ggsave2(fig_path(".pdf"))

include_graphics(fig_path(".pdf"))
```

The main effect parameters in the LCDM are constrained to be positive, thus ensuring monotonicity in the model [e.g., masters always have a higher probability of providing a correct response; @lcdm]. Thus, the attribute-level main effect for DLM models, $\lambda_{1,1}$ uses a lognormal prior, $\text{Lognormal}(\mu = 0, \sigma = 1)$. Similar to the attribute-level intercept, this distribution was chosen as 99% of the distribution covers the range of plausible values. Specifically, the lower 99% of this distribution covers the log-odds range of 0 to 10.24. An upper limit of approximately 10 was desired as a main effect of 10 would allow for an estimated probability of providing a correct response near 1.0 in the extreme case where the intercept was -5 (the lower tail of the attribute-level intercept prior distribution).

The distributions for these parameters are visualized in Figure \@ref(fig:attr-prior-dist).

```{r attr-prior-dist, fig.cap = "Prior distributions for attribute-level effects."}
bind_rows(
  tibble(.variable = "lambda[0]", x = seq(-10, 10, by = 0.01)) %>%
    mutate(y = dnorm(x, mean = 0, sd = 2)),
  tibble(.variable = "lambda[list(1,1)]", x = seq(0, 10, by = 0.01)) %>%
    mutate(y = dlnorm(x, meanlog = 0, sdlog = 1))
) %>%
  ggplot(aes(x = x, y = y)) +
    facet_wrap(~ .variable, nrow = 1, scales = "free", labeller = label_parsed) +
    geom_line() +
    labs(x = "Parameter Value", y = "Density") +
    theme_atlas_ms(strip_text_size = 20) -> plot

plot %>%
  ggsave2(fig_path(".png")) %>%
  ggsave2(fig_path(".pdf"))

include_graphics(fig_path(".pdf"))
```

## Prior Specification for Item-Level Effects {#item-priors}

The prior distributions for the item-level effects, $b_{i,0}$ and $b_{i,1,1}$, are determined by the type of model that is being estimated. For this proof of concept, three models are considered: fungible, non-fungible, and partial equivalency.

In the fungible model, it is assumed that all items measuring the attribute have the same item parameters. That is, the item-level effects are equivalent to the attribute-level effect. Thus, the item-level deviations from the attribute-level effects are all equal to 0. Conceptually, this means using a $\mathcal{N}(\mu=0,\ \sigma=0)$ prior for all $b_{i,0}$ and $b_{i,1,1}$ terms. In practice, to increase computational efficiency, these terms are left out of the model, and only the attribute-level effects are estimated.

In contrast, the non-fungible model assume that the item parameters are independent of one another. In other words, the parameters for one item do not dictate the parameters of other items. Conceptually, this means that the item-level deviations from the attribute-level effects are unconstrained, and thus an infinite uniform prior, $\mathcal{U}(-\infty,\ +\infty)$, would be used for all $b_{i,0}$ and $b_{i,1,1}$ terms. In practice, it is more efficient to directly estimate individual parameters for each item, rather than attribute-level effects with unconstrained item-level deviations. Therefore, this model more closely resembles a true LCDM in equation \@ref(eq:meas-lcdm), with the $\lambda_{i,0}$ and $\lambda_{i,1,1}$ parameters using the prior distributions described for the [attribute-level priors](#attr-priors).

The partial equivalency model represents a compromise between the extremes of the fungible and non-fungible models. In this model, item-level parameters are not entirely independent, but are also not constrained to be equivalent. Instead, the item-levels are assumed to come from some distribution of deviations. The smaller the variance of the distribution, the more fungible the items are. Conversely, a large variance would correspond to less fungibility. Conceptually and in practice, this model is similar to multilevel models, the item-level deviations use a hierarchical normal prior, $\mathcal{N}(\mu=0,\ \sigma)$, where $\sigma$ is an estimated parameter in the model. The $\sigma$ parameter uses a half-Student's *t*-distribution with $df = 3$ (Figure \@ref(fig:sigma-prior)). This prior ensures that the variance is always positive, and also allows for larger variances than a normal distribution would. However, the variances are also constrained to reasonable values (i.e., less than ~5).

```{r sigma-prior, fig.cap = "Prior distribution for hierarchical variance prior."}
ggplot(data = tibble(x = c(0, 6)), aes(x = x)) +
  stat_function(fun = dt, n = 500, args = list(df = 3)) +
  geom_segment(x = 0, y = 0, xend = 0, yend = dt(0, df = 3)) +
  labs(x = "Parameter Value", y = "Density") +
  theme_atlas_ms() -> plot

plot %>%
  ggsave2(fig_path(".png")) %>%
  ggsave2(fig_path(".pdf"))

include_graphics(fig_path(".pdf"))
```

## Prior Specification for Class-Level Parameters {#strc-priors}

The last parameter that requires a prior is the structural parameter in equation \@ref(eq:dcm), $\nu_c$. This parameter defines the base rate of inclusion for each class. As such, $\nu$ is constrained such that all elements sum to one (i.e., there are no non-class respondents). Because this discussion is limited to models with a single binary attribute, there are only two classes, and therefore two elements of $\nu$. No assumptions are made about the base rate of mastery for attributes; therefore, a uniform Dirichlet prior, $\text{Dir}(1)$, was used for the prior distribution. As there only two classes, this is equivalent to using a uniform Beta distribution, $\text{Beta}(\alpha=1,\ \beta=1)$, for $\nu_1$ and then calculating $\nu_2$ as $\nu_2 = 1 - \nu_1$.

# The Bayesian Framework in Practice

In order to demonstrate the utility and benefits of using the Bayesian model definition and estimation process, a single simulated data set was generated. This data set was then used to walk through each step of the Bayesian model fit process, from model estimation to model evaluations and comparisons. All analyses were performed in *R* version `r getRversion()` [@R-base].

## Simulated Data

```{r example-data, include = FALSE, cache = TRUE}
set.seed(9416)
num_stu <- 1700
att_mastery <- 0.6
ye_pct <- 0
rt_pct <- 0.1

# simulate items
items <- tibble(testlet_id = 101:104,
       window = rep(c("IE", "SP"), each = 2)) %>%
  mutate(num_item = sample(3:5, n(), replace = TRUE)) %>%
  uncount(weights = num_item, .id = "testlet_item_id") %>%
  rowid_to_column(var = "item_id") %>%
  mutate(attr_intercept = runif(1, -2.25, -1.00),
         attr_maineffect = runif(1, 1.00, 4.50),
         item_intercept = rnorm(n(), mean = 0, sd = 1.0),
         item_maineffect = trunc_sample(rnorm, n = n(),
                                        lb = -1 * unique(attr_maineffect),
                                        mean = 0, sd = 1.0),
         intercept = attr_intercept + item_intercept,
         maineffect = attr_maineffect + item_maineffect,
         nm_prob = map_dbl(intercept, inv_logit),
         ms_prob = map_dbl(intercept + maineffect, inv_logit))  %>%
  write_csv(here("data", "item-parameters.csv"))

all_response <- tibble(testlet_id = c("103", "104", "101,103", "101,104",
                                      "102,103", "102,104", "101,102,103",
                                      "101,102,104")) %>%
  mutate(testlets = str_count(testlet_id, ","),
         weight = case_when(testlets == 0 ~ 0.5 * ye_pct,
                            testlets == 1 ~ ((1 - rt_pct) * (1 - ye_pct)) / 4,
                            testlets == 2 ~ (rt_pct * (1 - ye_pct)) / 2)) %>%
  filter(weight > 0) %>%
  select(-testlets) %>%
  sample_n(size = num_stu, replace = TRUE, weight = weight) %>%
  select(-weight) %>%
  rowid_to_column(var = "stu_id") %>%
  mutate(mastery = sample(c(0L, 1L), n(), replace = TRUE,
                          prob = c(1 - att_mastery, att_mastery))) %>%
  separate_rows(testlet_id, convert = TRUE) %>%
  left_join(
    items %>%
      select(testlet_id,  item_id, nm_prob, ms_prob),
    by = "testlet_id"
  ) %>%
  mutate(prob_correct = (mastery * ms_prob) + ((1 - mastery) * nm_prob),
         rand = runif(n(), 0, 1),
         score = case_when(rand <= prob_correct ~ 1L, TRUE ~ 0L)) %>%
  write_rds(here("data", "all_response.rds"))

mastery <- distinct(all_response, stu_id, mastery) %>%
  write_csv(here("data", "student-parameters.csv"))

# Format data for Stan
response_matrix <- all_response %>%
  select(stu_id, item_id, score) %>%
  arrange(stu_id, item_id)
ragged_array <-  response_matrix %>%
  rowid_to_column() %>%
  group_by(stu_id) %>%
  summarize(start = min(rowid), num = n())
stan_data = list(
  I = nrow(items),
  J = num_stu,
  N = nrow(response_matrix),
  ii = response_matrix$item_id,
  jj = response_matrix$stu_id,
  y = response_matrix$score,
  s = ragged_array$start,
  l = ragged_array$num
)
```

When simulating the example data set, the data was structured similar to the DLM assessments. That is, items were grouped together into testlets of three to five items, and testlets were assigned to either the instructionally embedded or spring testing window. For more information on these testing windows and how content is assigned in practice, see Chapter 3 and Chapter 4 of @dlm_tech_1415_im. Item parameters were simulated according to the partial equivalency model defined in equation \@ref(eq:dlm-lcdm). The attribute-level intercept, $\lambda_0$ was drawn from a $\mathcal{U}(-2.25, -1.00)$ distribution, and the attribute-level main effect, $\lambda_{1,1}$ from a $\mathcal{U}(1.00, 4.50)$. The item-level deviations $b_{i,0}$ and $b_{i,1,1}$ were drawn from a $\mathcal{N}(\mu=0,\ \sigma = 1.0)$ distribution. This resulted in total item intercepts and main effects consistent with those reported for the DLM assessments [see Chapter 5 of @dlm_tech_1516_im]. The true parameter values for each testlet and item that were used to simulate the data can be seen in Table \@ref(tab:true-item-param).

```{r true-item-param}
items %>%
  select(-testlet_item_id, -nm_prob, -ms_prob) %>%
  mutate_if(is.double, ~sprintf("%0.2f", .)) %>%
  kable(align = c("c", "c", "c", rep("r", 4), "c", "c"), booktabs = TRUE,
        linesep = "", escape = FALSE, caption = "True Item Parameters",
        col.names = c("Item", "Testlet", "Window",
                      "$\\pmb{\\lambda_0}$",
                      "$\\pmb{\\lambda_{1,1}}$", "$\\pmb{b_{i,0}}$",
                      "$\\pmb{b_{i,1,1}}$", "$\\pmb{\\lambda_0 + b_{i,0}}$",
                      "$\\pmb{\\lambda_{1,1} + b_{i,1,1}}$")) %>%
  kable_styling(latex_options = "HOLD_position", position = "left") %>%
  row_spec(0, bold = TRUE, align = "c") %>%
  footnote(general = "IE = instructionally embedded; SP = spring.",
           footnote_as_chunk = TRUE)
```

To mimic the DLM test structure, students were pseudo-randomly assigned a combination of the simulated testlets. Following the test administration design for the instructionally embedded DLM testing model [for details see Chapter 4 of @dlm_tech_1415_im], students were assigned testlets from both the instructionally embedded and spring pools. When assigning spring assessments, students were randomly assigned only one testlet. For instructionally embedded assessments, there was `r str_extract(indefinite((1 - rt_pct) * 100), "\\w+")` `r (1 - rt_pct) * 100`% chance of taking only one testlet, and `r str_extract(indefinite(rt_pct * 100), "\\w+")` `r rt_pct * 100`% chance of taking both testlets. This is consistent with the reported usage of the instructionally embedded assessment window [@ie_usage]. The resulting probabilities for each possible combination of assigned testlets can be seen in Table \@ref(tab:testlet-prob), along with the total number of students actually simulated to have that combination. In total `r prettyNum(num_stu, big.mark = ",")` students were simulated, consistent with the total number of students that test on a single attribute in a given year from states participating in the instructionally embedded assessment model [see Chapter 7 of @dlm_tech_1415_im].

```{r testlet-prob}
tibble(testlet_id = c("103", "104", "101,103", "101,104", "102,103", "102,104",
                      "101,102,103", "101,102,104")) %>%
  mutate(testlets = str_count(testlet_id, ","),
         weight = case_when(testlets == 0 ~ 0.5 * ye_pct,
                            testlets == 1 ~ ((1 - rt_pct) * (1 - ye_pct)) / 4,
                            testlets == 2 ~ (rt_pct * (1 - ye_pct)) / 2)) %>%
  filter(weight > 0) %>%
  select(-testlets) %>%
  left_join(
    all_response %>%
      select(stu_id, testlet_id) %>%
      group_by(stu_id) %>%
      summarize(testlet_id = paste(sort(unique(testlet_id)), collapse = ",")) %>%
      count(testlet_id),
    by = "testlet_id"
  ) %>%
  mutate(testlet_id = str_replace_all(testlet_id, ",", ", "),
         weight = sprintf("%0.3f", weight),
         n = prettyNum(n, big.mark = ",")) %>%
  kable(align = c("c", "c", "r"), booktabs = TRUE, linesep = "", escape = FALSE,
        col.names = c("Testlet Combination", "Probability", "\\textit{n}"),
        caption = "Number of Simulated Students Assigned to Each Testlet Combination") %>%
  kable_styling(latex_options = "HOLD_position", position = "left") %>%
  row_spec(0, bold = TRUE, align = "c")
```

## Model Estimation {#estimate}

```{r estimate-models, dependson = "example-data", cache = TRUE, include = FALSE}
chains <- 4
iter <- 2000
warmup <- 1000

set.seed(1992)
fung_init <- map(seq_len(chains), function(x, num) {
  list(
    mean_intercept = runif(1, -2.25, -1.00),
    mean_maineffect = runif(1, 1.00, 4.50)
  )
})
pteq_init <- map(seq_len(chains), function(x, num) {
  list(
    mean_intercept = runif(1, -2.25, -1.00),
    mean_maineffect = runif(1, 1.00, 4.50),
    intercept_dev = runif(num, -0.5, 0.5),
    maineffect_dev = runif(num, -0.5, 0.5)
  )
}, num = nrow(items))
nfng_init <- map(seq_len(chains), function(x, num) {
  list(
    intercept = runif(num, -2.25, -1.00),
    maineffect = runif(num, 1.00, 4.50)
  )
}, num = nrow(items))

if (file_exists(here("data", "estimated-models", "fung.rds"))) {
  fung <- read_rds(here("data", "estimated-models", "fung.rds"))
} else {
  fung <- stan(here("Stan", "lca_fungible.stan"), data = stan_data,
             init = fung_init, chains = chains, iter = iter, warmup = warmup,
             cores = chains, refresh = 0, seed = 924,
             control = list(adapt_delta = 0.99, max_treedepth = 15))
  write_rds(fung, here("data", "estimated-models", "fung.rds"), compress = "gz")
}

if (file_exists(here("data", "estimated-models", "pteq.rds"))) {
  pteq <- read_rds(here("data", "estimated-models", "pteq.rds"))
} else {
  pteq <- stan(here("Stan", "lca_parteqest.stan"), data = stan_data,
             init = pteq_init, chains = chains, iter = iter, warmup = warmup,
             cores = chains, refresh = 0, seed = 924,
             control = list(adapt_delta = 0.99, max_treedepth = 15))
  write_rds(pteq, here("data", "estimated-models", "pteq.rds"), compress = "gz")
}

if (file_exists(here("data", "estimated-models", "nfng.rds"))) {
  nfng <- read_rds(here("data", "estimated-models", "nfng.rds"))
} else {
  nfng <- stan(here("Stan", "lca_nonfungible.stan"), data = stan_data,
             init = nfng_init, chains = chains, iter = iter, warmup = warmup,
             cores = chains, refresh = 0, seed = 924,
             control = list(adapt_delta = 0.99, max_treedepth = 15))
  write_rds(nfng, here("data", "estimated-models", "nfng.rds"), compress = "gz")
}
```

```{r ppmc-samples, dependson = "estimate-models", include = FALSE}
# ppmc
if (file_exists(here("data", "estimated-models", "fung_ppmc.rds"))) {
  fung_ppmc <- read_rds(here("data", "estimated-models", "fung_ppmc.rds"))
} else {
  fung_ppmc <- rep_data(fung, obs = response_matrix) %>%
    write_rds(here("data", "estimated-models", "fung_ppmc.rds"))
}

if (file_exists(here("data", "estimated-models", "pteq_ppmc.rds"))) {
  pteq_ppmc <- read_rds(here("data", "estimated-models", "pteq_ppmc.rds"))
} else {
  pteq_ppmc <- rep_data(pteq, obs = response_matrix) %>%
    write_rds(here("data", "estimated-models", "pteq_ppmc.rds"))
}

if (file_exists(here("data", "estimated-models", "nfng_ppmc.rds"))) {
  nfng_ppmc <- read_rds(here("data", "estimated-models", "nfng_ppmc.rds"))
} else {
  nfng_ppmc <- rep_data(nfng, obs = response_matrix) %>%
    write_rds(here("data", "estimated-models", "nfng_ppmc.rds"))
}

# compare
if (all(file_exists(here("data", "estimated-models",
                         glue("{c('fung_loo', 'fung_waic')}.rds"))))) {
  fung_loo <- read_rds(here("data", "estimated-models", "fung_loo.rds"))
  fung_waic <- read_rds(here("data", "estimated-models", "fung_waic.rds"))
} else {
  fung_log_lik <- extract_log_lik(fung)
  fung_loo <- loo(fung) %>%
    write_rds(here("data", "estimated-models", "fung_loo.rds"))
  fung_waic <- waic(fung_log_lik) %>%
    write_rds(here("data", "estimated-models", "fung_waic.rds"))
}

if (all(file_exists(here("data", "estimated-models",
                         glue("{c('pteq_loo', 'pteq_waic')}.rds"))))) {
  pteq_loo <- read_rds(here("data", "estimated-models", "pteq_loo.rds"))
  pteq_waic <- read_rds(here("data", "estimated-models", "pteq_waic.rds"))
} else {
  pteq_log_lik <- extract_log_lik(pteq)
  pteq_loo <- loo(pteq) %>%
    write_rds(here("data", "estimated-models", "pteq_loo.rds"))
  pteq_waic <- waic(pteq_log_lik) %>%
    write_rds(here("data", "estimated-models", "pteq_waic.rds"))
}

if (all(file_exists(here("data", "estimated-models",
                         glue("{c('nfng_loo', 'nfng_waic')}.rds"))))) {
  nfng_loo <- read_rds(here("data", "estimated-models", "nfng_loo.rds"))
  nfng_waic <- read_rds(here("data", "estimated-models", "nfng_waic.rds"))
} else {
  nfng_log_lik <- extract_log_lik(nfng)
  nfng_loo <- loo(nfng) %>%
    write_rds(here("data", "estimated-models", "nfng_loo.rds"))
  nfng_waic <- waic(nfng_log_lik) %>%
    write_rds(here("data", "estimated-models", "nfng_waic.rds"))
}
```

The models are estimated in *R* using the **rstan** package interface [@R-rstan] to *Stan* [@stan], which utilizes the No-U-Turn sampler [for a detailed explanation of the approach see @nuts]. The *Stan* code for all models can be found in the [online repository for this report](https://github.com/atlas-aai/bayes-concept). The models were estimated with `r words(chains)` chains, each with `r prettyNum(iter, big.mark = ",")` iterations. The first `r prettyNum(warmup, big.mark = ",")` iterations of each chain were discarded for warm-up, leaving a total of `r prettyNum((iter - warmup) * chains, big.mark = ",")` retained iterations that make up the posterior distributions. There are also several settings specific to the No-U-Turn Sampler [@nuts] used by *Stan*. First, the adaptive threshold was set to 0.99 to avoid divergent transitions [@betancourt_diverge]. Secondly, the maximum tree depth, which determines how far the algorithm can go before making a U-Turn [@betancourt_rstan], was set to 15. These, are both more conservative than the values suggested by the @stan_user, and the implications of these setting are discussed in the following sections, along with diagnostics to assess their impact.

After estimating the model, before the parameters can be analyzed and inferences can be made, the model is checked to ensure the estimation process completed in an appropriate manner. This diagnostic information is critical to Markov Chain Monte Carlo (MCMC) estimation, as without proper estimation, no valid inferences can be made. Checks include evaluating convergence, efficiency of the sampler, and parameter recovery. Each are described in detail below using the estimated partial equivalency model as an example, as this was the true data generating model.

### Convergence {#converge}

A check of convergence evaluates whether the MCMC chain successfully found the high density area of the posterior distribution and stayed there. When multiple chains are estimated, this can be checked by verifying that the estimates from each chain end up in the same parameter space. For a single chain, this is checked by verifying that the parameter is sampled from roughly the same area at the beginning of the chain (after warm-up) as it is at the end of the chain. This is commonly assessed through trace plots, an example of which is shown in Figure \@ref(fig:exm-trace).

(ref:exm-trace-cap) Trace plot for the attribute-level intercept $\lambda_0$.

```{r exm-trace, dependson = "estimate-models", fig.cap = "(ref:exm-trace-cap)"}
gather_draws(pteq, mean_intercept) %>%
  ggplot(aes(x = .iteration, y = .value, color = factor(.chain))) +
    geom_line() +
    scale_color_OkabeIto() +
    labs(x = "Iteration", y = expression(lambda[0]), color = "Chain") +
    theme_atlas_ms() -> plot

plot %>%
  ggsave2(fig_path(".png")) %>%
  ggsave2(fig_path(".pdf"))

include_graphics(fig_path(".pdf"))
```

Figure \@ref(fig:exm-trace) shows the trace plot for the attribute-level intercept, $\lambda_0$, and looks how a trace plot is expected to look. The draws appear to be coming from a stable distribution (i.e., the plot is relatively horizontal with no large upward or downward swings), and all `r words(chains)` are mixing well (as evidenced by the overlap of the `r words(chains)` colors). However, there is no empirical method that uses visual inspection alone to determine how poor a trace plot must be to conclude convergence was not met. Additionally, when there are many parameters, it is impractical to look at each individual trace plot.

To address these shortcomings of evaluating trace plots directly, the $\widehat{R}$ statistic can be used to evaluate convergence [@rhat; @new_rhat]. The $\widehat{R}$ statistic is also known as the potential scale reduction [@bda3], and is a measure of how much variance there is between chains relative to the amount of variation within chains. @rhat_cut suggest that in order to conclude that the model has successfully converged, all $\widehat{R}$ values should be less than 1.1. These results can be summarized, as in Figure \@ref(fig:rhat), to demonstrate the $\widehat{R}$ values for the estimated parameters. In the estimated partial equivalency model, all values are below 1.1, indicating that the model has converged.

(ref:rhat-cap) $\widehat{R}$ values for the estimated parameters in the partial equivalency model.

```{r rhat, fig.cap = "(ref:rhat-cap)"}
parms <- c("mean_intercept", "mean_maineffect", "intercept_dev",
           "maineffect_dev", "intercept_sd", "maineffect_sd", "nu")
labls <- c(expression(lambda[0]), expression(lambda[list(1,1)]),
           expression(italic(b)[list(i,0)]), expression(italic(b)[list(i,1,1)]),
           expression(sigma[italic(b)[list(i,0)]]),
           expression(sigma[italic(b)[list(i,1,1)]]), expression(nu[c]))

sims <- as.array(pteq)

apply(sims, MARGIN = 3, FUN = Rhat) %>%
  enframe(name = "parameter", value = "Rhat") %>%
  mutate(parameter = str_replace_all(parameter, "\\[.*\\]", "")) %>%
  filter(parameter %in% c("nu", "mean_intercept", "mean_maineffect",
                          "intercept_dev", "maineffect_dev", "intercept_sd",
                          "maineffect_sd")) %>%
  mutate(parameter = factor(parameter, levels = parms)) %>%
  ggplot(aes(x = parameter, y = Rhat, color = parameter)) +
    geom_jitter(size = 3, height = 0, width = 0.2, show.legend = FALSE) +
    geom_hline(yintercept = 1.1, linetype = "dashed") +
    scale_x_discrete(labels = labls) +
    scale_color_OkabeIto() +
    labs(x = NULL, y = expression(widehat(R))) +
    theme_atlas_ms() -> plot

plot %>%
  ggsave2(fig_path(".png")) %>%
  ggsave2(fig_path(".pdf"))

include_graphics(fig_path(".pdf"))
```

### Efficiency

A second check of the MCMC estimation is the efficiency of the sampler, which verifies that the algorithm adequately sampled the full posterior distribution. There are several ways this can be examined. The first is by examining the effective sample size. This diagnostic takes into account the autocorrelation (or anticorrelation) within chains to determine the "effective" number of independent draws from the posterior. If the chain is slow moving, the draws will be highly autocorrelated, and effective sample size will be well below the total number of retained iterations [@auto_corr]. Conversely, if the chain is moving quickly, it is possible for the draws to be better than independent, or anticorrelated [@anti_corr]. In this scenario, the effective sample size is actually larger than the true sample size.

There are two type of effective sample size that can be used to evaluate the efficiency of the location and scale of the posterior distributions. The sampling efficiency of the location (e.g., mean or median) can be assessed with the bulk effective sample size. Similarly, the scale can be assessed through tail effective sample size. This can be useful for diagnosing problems with mixing due posterior samples having different scales across chains [@new_rhat]. For both measures, the @stan_best_practice recommend an effective sample size greater than or equal to the number of chains times 100.

The effective sample size for all parameters in the model can be summarized, as in Figure \@ref(fig:eff-size). Because the model was estimated with `r words(chains)` chains, the effective sample size should be above `r prettyNum(chains * 100, big.mark = ",")`. Figure \@ref(fig:eff-size) shows that all parameters in the estimated partial equivalency model have both a bulk and tail effective sample size above this threshold.

```{r eff-size, fig.cap = "Effective sample size for each estimated parameter. ESS = effective sample size.", fig.scap = "Effective sample size for each estimated parameter."}
bulk_ess <- apply(sims, MARGIN = 3, FUN = ess_bulk)
tail_ess <- apply(sims, MARGIN = 3, FUN = ess_tail)

ess <- list(
  summary(pteq)$summary %>%
    as_tibble(rownames = "parameter") %>%
    select(parameter, n_eff),
  enframe(apply(sims, MARGIN = 3, FUN = ess_bulk), "parameter", "bulk_ess"),
  enframe(apply(sims, MARGIN = 3, FUN = ess_tail), "parameter", "tail_ess")
)

reduce(ess, full_join, by = "parameter") %>%
  mutate(parameter = str_replace_all(parameter, "\\[.*\\]", "")) %>%
  filter(parameter %in% c("nu", "mean_intercept", "mean_maineffect",
                          "intercept_dev", "maineffect_dev", "intercept_sd",
                          "maineffect_sd")) %>%
  mutate(parameter = factor(parameter, levels = parms)) %>%
  gather(key = "measure", value = "value", -parameter) %>%
  mutate(measure = case_when(measure == "n_eff" ~ "ESS",
                             measure == "bulk_ess" ~ "Bulk ESS",
                             measure == "tail_ess" ~ "Tail ESS")) %>%
  filter(measure != "ESS") %>%
  ggplot(aes(x = parameter, y = value, color = parameter)) +
    facet_wrap(~ measure, nrow = 1) +
    geom_jitter(size = 3, height = 0, width = 0.2, show.legend = FALSE) +
    geom_hline(yintercept = chains * 100, linetype = "dashed") +
    expand_limits(y = c(0, (iter - warmup) * chains)) +
    scale_x_discrete(labels = labls) +
    scale_y_comma() +
    scale_color_OkabeIto() +
    labs(x = NULL, y = "Effective Sample Size") +
    theme_atlas_ms() -> plot

plot %>%
  ggsave2(fig_path(".png")) %>%
  ggsave2(fig_path(".pdf"))

include_graphics(fig_path(".pdf"))
```

There are also measures of efficiency that are exclusive to the No-U-Turn Sampler [@nuts]. For example, the Bayesian factor of missing information gives and estimate of how well the sampler adapted and explored the posterior distribution. The Bayesian factor of missing information generally ranges from zero to one, with zero and one representing poor and excellent estimation, respectively. This is calculated for each chain overall, rather than each individual parameter [@bfmi].

The Bayesian factor of missing information values for this example are shown in Table \@ref(tab:efficiency) and indicate that the sample was able to adequately visit the posterior distributions. Additionally, Table \@ref(tab:efficiency) shows the mean acceptance rate for each chain. As expected, these values are very close to the 0.99 adaptive threshold that was specified during the [model estimation](#estimate). As mentioned previously, a target acceptance rate this high is needed to prevent divergent transitions.

The concern with setting the target acceptance rate this high is that for parameters with wider posteriors, the sampler will not be able to move fast enough. In the No-U-Turn Sampler, at each iteration, the sampler looks for a place to "U-Turn" in a series of possible branches. If the sample is terminating before the maximum possible tree depth (which was specified to be 15), then the algorithm is able to adequately find good values for the next iteration of the chain, despite the small steps being enforced by the high target acceptance rate. Bumping up against the maximum allowed tree depth, or going beyond it, indicates that the step size is too small [@stan_user; @stan_warn]. Because the maximum tree depth values in Table \@ref(tab:efficiency) are all below the maximum specified, and the Bayesian factor of missing information values are all close to one, there is strong evidence that in this model, the sampler was able to adequately sample the posteriors.

```{r efficiency}
sampler_params <- get_sampler_params(pteq, inc_warmup = FALSE)
upars <- suppressMessages(stan(here("Stan", "lca_parteqest.stan"),
                               data = stan_data, chains = 0)) %>%
  get_num_upars()
E <- as.matrix(sapply(sampler_params, FUN = function(x) x[, "energy__"]))
EBFMI <- upars / apply(E, 2, var)
mean_accept <- sapply(sampler_params, function(x) mean(x[, "accept_stat__"]))
max_treedepth <- sapply(sampler_params, function(x) max(x[, "treedepth__"]))

tibble(chain = glue("Chain {seq_len(chains)}"),
       bfmi = EBFMI,
       mean_accept = mean_accept,
       max_treedepth = as.integer(max_treedepth)) %>%
  mutate_if(is.double, ~ sprintf("%0.3f", .)) %>%
  kable(align = "c", booktabs = TRUE, linesep = "",
        caption = "Diagnostic Statistics for the No-U-Turn Sampler",
        col.names = c("Chain", "BFMI", "Mean Acceptance Rate",
                      "Max Tree Depth")) %>%
  kable_styling(latex_options = "HOLD_position", full_width = TRUE) %>%
  row_spec(0, bold = TRUE, align = "c") %>%
  footnote(general = "BFMI = Bayesian factor of missing information.",
           footnote_as_chunk = TRUE)
```

### Parameter Recovery

In addition to diagnostics to ensure that the model is estimated properly, it is also important to established that the model as defined in the *Stan* code is able to accurately recover the true parameter values. Otherwise, a model may estimate well, but actually be misspecified, leading to incorrect parameter estimates. Figure \@ref(fig:item-recover) shows the true (from Table \@ref(tab:true-item-param)) versus estimated item parameter values, indicating successful parameter recovery for the partial equivalency model with the simulated data.

```{r item-recover, fig.cap = "Parameter recovery from the example partial equivalency model with simulated data."}
summary(pteq)$summary %>%
  as_tibble(rownames = "parameter") %>%
  filter(str_detect(parameter, "(intercept|maineffect)\\[")) %>%
  select(parameter, est = mean) %>%
  separate(parameter, into = c("parameter", "item_id", NA), convert = TRUE) %>%
  left_join(
    items %>%
      select(item_id, intercept, maineffect) %>%
      gather(key = "parameter", value = "true", -item_id),
    by = c("parameter", "item_id")
  ) %>%
  mutate(
    parameter = factor(parameter, levels = c("intercept", "maineffect"),
                       labels = c("lambda[0] + italic(b)[list(i,0)]",
                                  "lambda[1,1] + italic(b)[list(i,1,1)]"))
  ) %>%
  ggplot(aes(x = true, y = est)) +
    facet_wrap(~ parameter, nrow = 1, scales = "free",
               labeller = label_parsed) +
    geom_point(size = 3) +
    geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
    labs(x = "True Value", y = "Estimated Value") +
    theme_atlas_ms() -> plot

plot %>%
  ggsave2(fig_path(".png")) %>%
  ggsave2(fig_path(".pdf"))

include_graphics(fig_path(".pdf"))
```

```{r class-recover-calc}
thresh <- 0.5
class_recovery <- mastery %>%
  rename(true = mastery) %>%
  left_join(
    map_dfr(pteq_ppmc$mastery_probs, function(x) return(x), .id = ".draw") %>%
      group_by(stu_id) %>%
      summarize(prob = median(master_prob)) %>%
      mutate(est = case_when(prob >= thresh ~ 1L, TRUE ~ 0L)) %>%
      select(stu_id, est),
    by = "stu_id"
  ) %>%
  count(true, est)

correct_rate <- class_recovery %>%
  mutate(correct = case_when(true == est ~ n, TRUE ~ 0L)) %>%
  summarize(correct = sum(correct) / sum(n)) %>%
  pull(correct)
```

It is also possible to examine the accuracy of the respondent classifications as a master or non-master. For this analysis, respondents were classified as masters if the median of the posterior distribution for the probability of mastery was greater than or equal to `r sprintf("%0.1f", thresh)`. This threshold places respondents in their most likely class; however, any threshold can be used in practice to facilitate stakeholder understanding of scores [@bradshaw_2019]. For example, DLM assessments use a threshold of 0.8 in order to have more confidence that students reported as masters are truly masters [see Chapter 5 of @dlm_tech_1516_im]. Respondent classification results for the partial equivalency model are summarized in Table \@ref(tab:class-recover). In total, `r sprintf("%0.0f", correct_rate * 100)`% of the simulated students were correctly classified as masters or non-masters. This is not surprising, given that the data was simulated to fit the model, and all of the items are fairly discriminating (Table \@ref(tab:true-item-param)).

```{r class-recover}
class_recovery %>%
  mutate(n = prettyNum(n, big.mark = ",")) %>%
  kable(align = c("c", "c", "r"), booktabs = TRUE, linesep = "", escape = FALSE,
        caption = "Respondent Classification Accuracy",
        col.names = c("True Mastery", "Estimated Mastery", "\\textit{n}")) %>%
  kable_styling(latex_options = "HOLD_position", position = "left") %>%
  row_spec(0, bold = TRUE, align = "c")
```

## Evaluating Model Fit

```{r clear-mem, include = FALSE}
rm(fung, pteq, nfng, stan_data); gc()
```

Model fit can be assessed in both an absolute and relative sense. Absolute fit is used to evaluate whether or not the estimated model adequately reflects the observed data and is a prerequisite for the evaluation of relative fit. Relative fit compares the fit of two more models that all show adequate absoulte model fit [@chen_2013; @sen_2017]. For the DLM assessments, absolute and relative fit are assessed through posterior predictive model checking and model comparisons, respectively. In order to demonstrate how these methods work in practice, both the fungible and non-fungible models were estimated on the same data used to estimate the partial equivalency model in the [previous section](#estimate). Thus, there are a total of three models to compare and calculate posterior predictive checks for. For all model fit analyses, recall that the partial equivalency model was the true data generation model.

### Absolute Fit {#abs-fit}

Posterior predictive model checks are used to assess the absolute fit of a specific model to the observed data. Posterior predictive checks involve simulating replications of the data using the values of the posterior distributions, and then comparing the replicated data sets back to the observed data [@bda3]. Recall from the [model estimation section](#estimate) section that a total of `r prettyNum((iter - warmup) * chains, big.mark = ",")` iterations were retained from the MCMC estimation. Thus `r prettyNum((iter - warmup) * chains, big.mark = ",")` replicated data sets can be simulated, one for each iteration, using the current values of the parameters at each iteration. The process for simulating a replicated data set for a single interation is as follows:

1. Randomly assign the first respondent to the master or non-master class, with probability equal to the current value of their probability of attribute mastery.
2. For the first item the the respondent took, simulate a response using the current values of the item parameters and the mastery status that was simulated in step 1.
3. Repeat step 2 for all items the respondent tested on.
4. Repeat steps 1-3 for all respondents.

This process is then repeated for each iteration in the chain. Because the replicated data sets are simulated from the current values of the parameters, these data sets represent what the data would be expected to look like *if the specified model were true*. Therefore, summaries of these data sets can then be used to look for systematic differences in the characteristics of the observed data and the replicated data sets, often through visualizations [@gelman_hill].

#### Model-Level Fit

```{r score-dist-calc, cache = TRUE, include = FALSE}
calc_score_dist <- function(x) {
  map_dfr(x$ppmc_data, function(x) {
    x %>%
      group_by(stu_id) %>%
      summarize(raw_score = sum(score)) %>%
      count(raw_score)
  }, .id = ".draw")
}

score_dist <- list(fung_ppmc, pteq_ppmc, nfng_ppmc) %>%
  set_names("Fungible", "Partial Equivalency", "Non-fungible") %>%
  map(calc_score_dist) %>%
  bind_rows(.id = "model") %>%
  complete(model, .draw, raw_score, fill = list(n = 0)) %>%
  mutate(model = factor(model, levels = c("Fungible", "Partial Equivalency",
                                          "Non-fungible")))

score_summary <- score_dist %>%
  group_by(model, raw_score) %>%
  summarize(mean = mean(n),
            median = median(n),
            lb = quantile(n, probs = 0.025),
            ub = quantile(n, probs = 0.975)) %>%
  left_join(
    response_matrix %>%
      group_by(stu_id) %>%
      summarize(raw_score = sum(score)) %>%
      count(raw_score),
    by = "raw_score"
  ) %>%
  rename(obs = n)
```

At the model-level (the linkage level for DLM assessments), posterior predictive checks can be calculated for the raw score distribution. This is accomplished by counting the number of respondents at each raw score point in each of the `r prettyNum((iter - warmup) * chains, big.mark = ",")` replicated data sets. In DLM assessments, this would be the total number of correct items for a student on a given linkage level. Thus, a distribution is derived from the number of respondents expected to be present at each raw score point in the observed data. Figure \@ref(fig:score-dist) shows the distribution of expected raw scores along with the number of observed students in the simulated data. This shows a bimodal distribution, which is a result of the mixture of raw score distributions for masters and non-masters. Additionally, there very few student expected to have a high raw score due to the fact that few students test on three testlets (Table \@ref(tab:testlet-prob)). Finally, the expected distribution for fungible model shows some deviations from the observed scores. Specifically, the fungible model over-estimates the number of students with a raw score of seven.

```{r score-dist, fig.cap = "Posterior predictive model check for the raw score distribution."}
ggplot() +
  facet_wrap(~ model, ncol = 1) +
  geom_jitter(data = group_by(score_dist, model, raw_score) %>% sample_n(500),
              aes(x = raw_score, y = n),
              alpha = 0.2, height = 0, width = 0.3) +
  geom_line(data = score_summary,
            aes(x = raw_score, y = lb, color = "95% Credible Interval"),
            linetype = "dashed", show.legend = FALSE) +
  geom_line(data = score_summary,
            aes(x = raw_score, y = ub, color = "95% Credible Interval"),
            linetype = "dashed", show.legend = FALSE) +
  geom_line(data = score_summary,
            aes(x = raw_score, y = obs, color = "Observed"),
            linetype = "solid") +
  geom_point(data = score_summary,
            aes(x = raw_score, y = obs, color = "Observed"),
            size = 4, show.legend = FALSE) +
  scale_x_continuous(breaks = seq(0, 15, 1)) +
  scale_y_comma(expand = waiver()) +
  scale_color_OkabeIto() +
  labs(x = "Raw Score", y = "Students", color = NULL) +
  theme_atlas_ms() -> plot

plot %>%
  ggsave2(fig_path(".png"), height = 8) %>%
  ggsave2(fig_path(".pdf"), height = 8)

include_graphics(fig_path(".pdf"))
```

Similar to the examination of trace plots [above](#converge) (Figure \@ref(fig:exm-trace)), this visualization alone is insufficient for determining if the amount of misfit in the distribution is significant. Rather, @beguin_2001 suggest a $\chi^2$ discrepancy measure can be calculated according to equation \@ref(eq:chisq).

\begin{equation}
  \chi_{obs}^2=\sum_{s=0}^S\frac{[n_s-E(n_s)]^2}{E(n_s)}
  (\#eq:chisq)
\end{equation}

In equation \@ref(eq:chisq), $s$ represents the score point, $n_s$ is the number of respondents at score point $s$, and $E(n_s)$ is the expected number of respondents at score point $s$, calculated as the average over all of the replicated data sets. Like the $\chi^2$ tests that are used to assess model fit when the expectation-maximization algorithm is used, the $\chi_{obs}^2$ statistic does not follow a true $\chi^2$ distribution. However, when using posterior predictive model checks, none of the distributional assumptions are required. This is because the reference distribution can be generated directly from the replicated data sets, similar to a parametric bootstrap. Using the same definition of $E(n_s)$ as above, a $\chi_{rep}^2$ can be computed for each of the replicated data sets. The `r prettyNum((iter - warmup) * chains, big.mark = ",")` $\chi_{rep}^2$ values then make up the reference distribution to compare back to $\chi_{obs}^2$. A posterior predictive *p*-value (*ppp*) can then be calculated as shown in equation \@ref(eq:ppp).

\begin{equation}
  ppp=P(\chi_{rep}^2\geq\chi_{obs}^2\ |\ n_s)
  (\#eq:ppp)
\end{equation}

In words, equation \@ref(eq:ppp) says that the posterior predictive *p*-value is the proportion of replicated data sets whose $\chi_{rep}^2$ value is greater than the $\chi_{obs}^2$ value from the observed data. Posterior predictive *p*-values close to zero indicate poor model fit (a cutoff of .05 could be used, for example), whereas values very close to one may indicate possible over-fitting. The $\chi_{obs}^2$ distributions and posterior predictive *p*-values for the fungible, partial equivalency, and non-fungible model are shown in Table \@ref(tab:chisq-stats) and visualized in Figure \@ref(fig:chisq-dist). As expected given the distributions in Figure \@ref(fig:score-dist), the fungible model shows poor model fit, with a posterior predictive *p*-value less than .05. In contrast, both the partial equivalency and non-fungible models show acceptable fit to the simulated data.

```{r chisq-calc, include = FALSE}
ppmc_chisq <- score_dist %>%
  left_join(select(score_summary, model, raw_score, exp = mean),
            by = c("model", "raw_score")) %>%
  mutate(exp = na_if(exp, 0)) %>%
  replace_na(list(exp = 0.000001)) %>%
  mutate(piece = ((n - exp) ^ 2) / exp) %>%
  group_by(model, .draw) %>%
  summarize(chisq = sum(piece))

obs_chisq <- response_matrix %>%
  group_by(stu_id) %>%
  summarize(raw_score = sum(score)) %>%
  count(raw_score) %>%
  full_join(select(score_summary, model, raw_score, exp = mean),
            by =  "raw_score") %>%
  mutate(exp = na_if(exp, 0)) %>%
  replace_na(list(exp = 0.000001, n = 0)) %>%
  mutate(piece = ((n - exp) ^ 2) / exp) %>%
  group_by(model) %>%
  summarize(obs_chisq = sum(piece)) %>%
  left_join(ppmc_chisq, by = "model") %>%
  group_by(model) %>%
  summarize(ppp = sprintf("%0.3f", mean(chisq >= obs_chisq)),
            obs_chisq = unique(obs_chisq),
            rep_mean = mean(chisq),
            rep_5 = quantile(chisq, probs = 0.05),
            rep_95 = quantile(chisq, probs = 0.95)) %>%
  mutate(sign = case_when(ppp == "1.000" ~ ">",
                          ppp == "0.000" ~ "<",
                          TRUE ~  "="),
         ppp = case_when(ppp == "1.000" ~ "0.999",
                         ppp == "0.000" ~ "0.001",
                         TRUE ~ ppp),
         lab = paste0(expression(italic(ppp)), '~"', sign, '"~', ppp))
```

(ref:chisq-stats-cap) $\chi_{obs}^2$ Values and Summaries of $\chi_{rep}^2$ Distributions.

(ref:chisq-stat-foot) *ppp* = posterior predictive *p*-value.

```{r chisq-stats}
obs_chisq %>%
  select(model, obs_chisq, rep_mean, rep_5, rep_95, ppp) %>%
  mutate_if(is.double, ~ sprintf("%0.2f", .)) %>%
  kable(align = c("l", "r", rep("c", 4)), booktabs = TRUE, linesep = "",
        escape = FALSE, caption = "(ref:chisq-stats-cap)",
        col.names = c("Model", "$\\pmb{\\chi_{obs}^2}$",
                      "$\\pmb{\\chi_{rep}^2}$ Mean",
                      "$\\pmb{\\chi_{rep}^2}$ 5\\%",
                      "$\\pmb{\\chi_{rep}^2}$ 95\\%",
                      "\\textit{ppp}")) %>%
  kable_styling(latex_options = "HOLD_position", position = "left") %>%
  row_spec(0, bold = TRUE, align = "c") %>%
  column_spec(1, width = "10em") %>%
  footnote(general = "(ref:chisq-stat-foot)", footnote_as_chunk = TRUE)
```

(ref:chisq-dist-cap) Posterior predictive model check for $\chi_{rep}^2$ distributions and $\chi_{obs}^2$ values.

```{r chisq-dist, fig.cap = "(ref:chisq-dist-cap)"}
ggplot() +
  facet_wrap(~ model, ncol = 1) +
  geom_histogram(data = ppmc_chisq,
                 aes(x = chisq, y = stat(density),
                     fill = "95% Credible Interval",
                     color = "95% Credible Interval"),
                 alpha = 0.8, binwidth = 1, boundary = 0) +
  geom_histogram(data = ppmc_chisq,
                 aes(x = chisq, y = stat(density), fill = "Observed"),
                 alpha = 0, binwidth = 1) +
  geom_line(data = ppmc_chisq, aes(x = chisq), stat = "density", trim = TRUE) +
  geom_vline(data = obs_chisq,
             aes(xintercept = obs_chisq, color = "Observed"),
             linetype = "dashed", size = 1, show.legend = FALSE) +
  # geom_text(data = obs_chisq,
  #           aes(x = 40, y = 0.003, label = lab),
  #           vjust = 0, hjust = 0, parse = TRUE) +
  expand_limits(x = c(0, 45)) +
  scale_fill_OkabeIto() +
  scale_color_OkabeIto() +
  labs(x = expression(chi[italic(rep)]^2), y = "Posterior Density", fill = NULL) +
  theme_atlas_ms() +
  guides(color = "none") -> plot

plot %>%
  ggsave2(fig_path(".png"), height = 8) %>%
  ggsave2(fig_path(".pdf"), height = 8)

include_graphics(fig_path(".pdf"))
```

#### Item-Level Fit

Posterior predictive checks can also be used to assess item-level fit by creating posterior summaries of item *p*-values. This is accomplished by calculating a *p*-value for each item in each of the `r prettyNum((iter - warmup) * chains, big.mark = ",")` replicated data sets. This provides a distribution for plausible item *p*-values if the model fits. The item *p*-values from the observed data can then be compared to these distributions. Figure \@ref(fig:ppmc-all-pval) shows an example of these comparisons for the fungible, partial equivalency, and non-fungible models. This shows very clearly the assumptions of the fungible model. That is, each item has the same expected *p*-value in the fungible model, but different expectations for the partial equivalency and non-fungible models. Because the simulated data was not fungible, Figure \@ref(fig:ppmc-all-pval) correctly shows that the fungible model was unable to successfully recover the observed item *p*-values.

```{r pval-calc, cache = TRUE, include = FALSE}
pval_calc <- function(master, rep, obs) {
  overall_pval <- rep %>%
    group_by(item_id) %>%
    summarize(ppmc_pval = mean(score)) %>%
    left_join(
      obs %>%
        group_by(item_id) %>%
        summarize(obs_pval = mean(score)),
      by = "item_id"
    )
  
  cond_pval <- obs %>%
    rename(obs_score = score) %>%
    left_join(rename(rep, sim_score = score), by = c("stu_id", "item_id")) %>%
    left_join(master, by = "stu_id") %>%
    mutate(master = case_when(master_prob >= 0.5 ~ "ms",
                              TRUE ~ "nm")) %>%
    group_by(item_id, master) %>%
    summarize(ppmc_pval = mean(sim_score),
              obs_pval=  mean(obs_score)) %>%
    pivot_longer(ppmc_pval:obs_pval, names_to = "meas", values_to = "value") %>%
    pivot_wider(names_from = c(master, meas), values_from = value)
  
  full_join(overall_pval, cond_pval, by = "item_id")
}

if (file_exists(here("data", "estimated-models", "pvals.rds"))) {
  all_pval <- read_rds(here("data", "estimated-models", "pvals.rds"))
} else {
  all_pval <- bind_rows(
    map2_dfr(fung_ppmc$mastery_probs, fung_ppmc$ppmc_data, pval_calc,
             obs = response_matrix, .id = ".draw") %>%
      mutate(.draw = as.integer(.draw), model = "Fungible"),
    map2_dfr(pteq_ppmc$mastery_probs, pteq_ppmc$ppmc_data, pval_calc,
             obs = response_matrix, .id = ".draw") %>%
      mutate(.draw = as.integer(.draw), model = "Partial Equivalency"),
    map2_dfr(nfng_ppmc$mastery_probs, nfng_ppmc$ppmc_data, pval_calc,
             obs = response_matrix, .id = ".draw") %>%
      mutate(.draw = as.integer(.draw), model = "Non-fungible"),
  ) %>%
  mutate(model = factor(model, levels = c("Fungible", "Partial Equivalency",
                                          "Non-fungible")))
  write_rds(all_pval, here("data", "estimated-models", "pvals.rds"),
            compress = "gz")
}
```

(ref:ppmc-all-pval-cap) Posterior predictive model check for overall item *p*-values.

```{r ppmc-all-pval, fig.cap = "(ref:ppmc-all-pval-cap)"}
pval_summary <- all_pval %>%
  group_by(model, item_id) %>%
  summarize(obs = unique(obs_pval),
            ppmc_lb = quantile(ppmc_pval, probs = 0.025),
            ppmc_mb = quantile(ppmc_pval, probs = 0.500),
            ppmc_ub = quantile(ppmc_pval, probs = 0.975))

ggplot() +
  facet_wrap(~ model, ncol = 1) +
  geom_jitter(data = group_by(all_pval, model, item_id) %>% sample_n(500),
              aes(x = item_id, y = ppmc_pval),
              alpha = 0.2, height = 0, width = 0.3) +
  geom_crossbar(data = pval_summary,
                aes(x = item_id, y = ppmc_mb, ymin = ppmc_lb, ymax = ppmc_ub,
                    color = "95% Credible Interval"),
                show.legend = FALSE) +
  geom_point(data = pval_summary,
             aes(x = item_id, y = obs, color = "Observed"),
             shape = 18, size = 3) +
  expand_limits(y = c(0, 1)) +
  scale_x_continuous(breaks = seq(1, nrow(items), 1)) +
  scale_color_OkabeIto() +
  labs(x = "Item", y = expression(Item~~italic(p)*-value), color = NULL) +
  theme_atlas_ms() +
  theme(panel.grid.minor.x = element_blank()) +
  guides(color = guide_legend(override.aes = list(shape = 15))) -> plot

plot %>%
  ggsave2(fig_path(".png"), height = 8) %>%
  ggsave2(fig_path(".pdf"), height = 8)

include_graphics(fig_path(".pdf"))
```

One limitation of using the overall item *p*-values is that this method over looks an important aspect of the model. Recall from equation \@ref(eq:dlm-lcdm) that the model actually defines two *conditional* probabilities, not one unconditional probability. In other words, the model defines a probability of providing a correct or non-masters, and a separate probability for masters. Thus, using a single *p*-value may miss important characteristics of the data. To examine this, posterior distributions for *p*-values conditional on mastery status can be estimated, similar to the overall *p*-value method.

This process is slightly complicated by that fact that mastery status is unobserved. Practically, this means that there is not an observed *p*-value for each mastery class to compare back to the posterior distributions readily available in the data. To calculate the *p*-value of an item for each mastery class, a procedure similar to that described by @sinharay_2007 is followed. When using a Bayesian MCMC estimation, a mastery classification can be made at each iteration of the Markov chain, as described [above](#abs-fit). Specifically, the probability of mastery is dichotomized at .5, although other thresholds can also be used. Using these classifications, a class-level *p*-value can be calculated for each iteration using the observed item responses and the respondents who were assigned to each class in that iteration. This results in a series of class-level *p*-values (one per item per iteration). The "observed" class-level *p*-values are then defined as the median of for each class and item across all iterations [see @sinharay_2007 for full details].

Similarly, an expected class-level *p*-value can be calculated by following the same procedure, but item responses generated in the replicated data sets, rather than those in the observed data. In this way, a distribution of expected conditional *p*-values can be estimated to compare the "observed" conditional *p*-values to. This comparison is shown in Figure \@ref(fig:ppmc-cond-pval). Much like the overall *p*-values (Figure \@ref(fig:ppmc-all-pval)), the fungible model shows consistent expectation across items for each mastery class which often miss the observed value. Additionally, the partial equivalency and non-fungible models successfully recover the observed conditional *p*-values for both classes. However, Figure \@ref(fig:ppmc-cond-pval) shows that the fungible model tends to miss *p*-values for the master class more often and by a greater magnitude than for the non-master class. Thus, it appears that there is more non-fungibility in item parameters among the master than non-master class for the data that was simulated for this document. This can also be seen when examining the true parameter values that were used to simulate the data Table \@ref(tab:true-item-param), which show wider variability for $\lambda_{1,1} + b_{i,1,1}$ than for $\lambda_0 + b_{i,0}$.

(ref:ppmc-cond-pval-cap) Posterior predictive model check for conditional item *p*-values.

```{r ppmc-cond-pval, fig.cap = "(ref:ppmc-cond-pval-cap)"}
cond_pval_summary <- all_pval %>%
  select(model, item_id, .draw, contains("ms"), contains("nm")) %>%
  pivot_longer(cols = c(-model, -item_id, -.draw)) %>%
  separate(name, into = c("class", "meas"), extra = "merge") %>%
  pivot_wider(names_from = meas, values_from = value) %>%
  group_by(model, item_id, class) %>%
  summarize(obs = median(obs_pval),
            ppmc_lb = quantile(ppmc_pval, probs = 0.025),
            ppmc_mb = quantile(ppmc_pval, probs = 0.500),
            ppmc_ub = quantile(ppmc_pval, probs = 0.975))

ggplot(cond_pval_summary) +
  facet_wrap(~ model, ncol = 1) +
  geom_rect(aes(xmin = item_id - 0.2, xmax = item_id + 0.2,
                ymin = ppmc_lb, ymax = ppmc_ub,
                fill = "95% Credible Interval",
                color = class),
            alpha = 0.7, size = 1) +
  geom_point(aes(x = item_id, y = obs, shape = "Observed",
                 color = class), size = 3) +
  scale_fill_OkabeIto() +
  scale_color_OkabeIto(order = c(3, 6), breaks = c("nm", "ms"),
                       labels = c("Non-master", "Master")) +
  scale_shape_manual(values = c(18)) +
  scale_x_continuous(breaks = seq(1, nrow(items), 1)) +
  labs(x = "Item", y = expression(Item~~italic(p)*-value),
       color = NULL, fill = NULL, shape = NULL) +
  theme_atlas_ms() +
  theme(panel.grid.minor.x = element_blank()) +
  guides(color = guide_legend(order = 3, override.aes = list(alpha = 0)),
         shape = guide_legend(order = 2),
         fill = guide_legend(order = 1, override.aes = list(alpha = 1))) -> plot

plot %>%
  ggsave2(fig_path(".png"), height = 8) %>%
  ggsave2(fig_path(".pdf"), height = 8)

include_graphics(fig_path(".pdf"))
```

#### Summary of Absolute Fit Analyses

For the assessment of absolute fit of the estimated models to the data, posterior predictive model checks were used to create expected distributions of data summary statistics (e.g., raw scores, *p*-values). At the model level, the raw score distributions and associated $\chi^2$-like statistics indicated that the fungible model did not fit the data whereas the partial equivalency and non-fungible models showed adequate recovery of the raw score distribution (Table \@ref(tab:chisq-stats)). At the item level, the fungible model again failed to recover characteristics of the observed data. Both the overall and conditional *p*-values had observed values that were consistently missed by the expectations produced by the fungible model. In contrast, both the partial equivalency and non-fungible models were able to recover both overall and conditional item *p*-values. Combined, these analyses provide strong evidence that only the partial equivalency and non-fungible models fit this example data.

### Relative Fit

```{r rel-fit-calc, include = FALSE}
loo_list <- list(pteq_loo, nfng_loo)
waic_list <- list(pteq_waic, nfng_waic)
names(loo_list) <- names(waic_list) <- c("pteq", "nfng")

relative_fit <- tibble(model = names(loo_list),
                       loo = loo_list,
                       waic = waic_list) %>%
  mutate(loo_est = map_chr(loo, function(x) {
    x$estimates %>%
      as_tibble(rownames = "meas") %>%
      filter(meas == "elpd_loo") %>%
      mutate(psis_loo = paste0(format(round(Estimate, 1), nsmall = 1,
                                      big.mark = ","),
                               " (", sprintf("%0.1f", SE), ")")) %>%
      pull(psis_loo)
  }),
         waic_est = map_chr(waic, function(x) {
    x$estimates %>%
      as_tibble(rownames = "meas") %>%
      filter(meas == "elpd_waic") %>%
      mutate(waic = paste0(format(round(Estimate, 1), nsmall = 1,
                                  big.mark = ","),
                           " (", sprintf("%0.1f", SE), ")")) %>%
      pull(waic)
  })) %>%
  left_join(
    loo_compare(loo_list) %>%
      as_tibble(rownames = "model") %>%
      mutate(loo_diff = paste0(sprintf("%0.1f", elpd_diff), " (",
                               sprintf("%0.1f", se_diff), ")")) %>%
      select(model, loo_diff),
    by = "model"
  ) %>%
  left_join(
    loo_compare(waic_list) %>%
      as_tibble(rownames = "model") %>%
      mutate(waic_diff = paste0(sprintf("%0.1f", elpd_diff), " (",
                                sprintf("%0.1f", se_diff), ")")) %>%
      select(model, waic_diff),
    by = "model"
  ) %>%
  left_join(
    enframe(loo_model_weights(loo_list), name = "model", value = "stacking") %>%
      mutate(stacking = sprintf("%0.4f", stacking),
             stacking = case_when(stacking == "1.0000" ~ ">.999",
                                  stacking == "0.0000" ~ "<.001",
                                  TRUE ~ stacking)),
    by = "model"
  ) %>%
  select(model, loo_est:stacking)
```

Relative fit is assessed through the comparison of models to determine if one model has relatively better fit than another. Because there is no direct comparison to the data with these methods, it is important that models show adequate absolute before they are compared to each other to determine the best fitting model [@sen_2017]. Therefore, the fungible model is excluded from these analyses, as adequate absolute model fit was not demonstrated in this example data.

For DLM assessments, three methods of comparison are used: Pareto smoothed importance sampling leave-one-out cross validation [PSIS-LOO; @loo_waic; @psis], the widely applicable information criterion [WAIC; @waic], and Bayesian stacking [@stacking]. Both the PSIS-LOO and WAIC provide point estimates for the out of sample prediction accuracy using the log-likelihood posterior distribution. However, although the PSIS-LOO and WAIC are asymptotically equivalent, @loo_waic found that the PSIS-LOO is more robust than the WAIC when weak priors are used (as is true for the models utilized by DLM) and when there are influential observations (e.g., a student providing an incorrect response despite a high probability of success). When comparing these indices in practice, a difference between models is considered significant if the absolute difference in the indices is greater than 2.5 times the standard error of the difference [@bengio_2004].

Additionally, model comparisons can be made using model stacking or averaging. These methods work by assigning weights to each model in the comparison that correspond to the weight that should be given to predictions from each model. Thus, the more weight assigned to a model, the more preferred it would be in isolation. This method has the benefit of being less prone to over-fitting [@piironen_2017] and also allowing for more refined inferences [@vehtari_2012]. For example, because the weights are on a probability scale, they are much easier to interpret and compare across models than the PSIS-LOO or WAIC. For DLM, the Bayesian stacking method described by @stacking is implemented.

Table \@ref(tab:rel-fit) shows that the PSIS-LOO and WAIC fit indices very similar (indeed identical to one decimal place) for both the partial equivalency and non-fungible models. This is not unexpected, as the PSIS-LOO and WAIC are asymptotically equivalent [@loo_waic]. In the model comparisons, both the PSIS-LOO and WAIC prefer the partial equivalency model, as indicated by the zero in the model comparison (the difference between that model and the preferred model). However, the difference in PSIS-LOO and WAIC of -1.1 between the partial equivalency and non-fungible model is less than 2.5 times the standard error of the difference (1.3). Thus, using the criteria outlined by @bengio_2004, these models fit equally well. In contrast Bayesian stacking shows a large preference for the partial equivalency. In this  comparison, nearly all of the weight is given to the partial equivalency model. Given the totality of these analyses, the partial equivalency model appears to be the model best suited to this data. Recall that this data was simulated from the partial equivalency model, so this finding is consistent with what is expected.

```{r rel-fit}
relative_fit %>%
  mutate(model = case_when(model == "pteq" ~ "Partial Equivalency",
                           model == "nfng" ~ "Non-fungible")) %>%
  kable(align = c("l", "c", "c", "r", "r", "c"), booktabs = TRUE, linesep = "",
        col.names = c("Model", "PSIS-LOO", "WAIC", "PSIS-LOO", "WAIC",
                      "Bayesian Stacking"),
        caption = "Relative Fit Indices and Model Comparisons.") %>%
  kable_styling(latex_options = "HOLD_position", position = "left") %>%
  row_spec(0, bold = TRUE, align = "c") %>%
  add_header_above(c(" " = 1, "Fit Indices" = 2, "Model Comparisons" = 3),
                   bold = TRUE) %>%
  footnote(general = "PSIS-LOO = Pareto smoothed importance sampling leave-one-out cross validation; WAIC = widely applicable information criterion. Parentheses represent the standard error of the index or comparison.", footnote_as_chunk = TRUE, threeparttable = TRUE)
```


# Discussion

Placeholder text.


\newpage

# References {-}

```{r write-packages, include = FALSE}
if (!file.exists("bib/packages.bib")) file.create("bib/packages.bib")
suppressWarnings(
  knitr::write_bib(c(.packages()), "bib/packages.bib")
)

# Correct capitalization in packages
read_lines("bib/packages.bib") %>%
  str_replace_all(" Stan", " {Stan}") %>%
  str_replace_all("rstan:", "{RStan}:") %>%
  write_lines("bib/packages.bib")
```

\printbibliography[heading=none]

\setlength{\parindent}{15pt}
\setlength{\leftskip}{0pt}
